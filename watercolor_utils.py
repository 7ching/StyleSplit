import cv2
import numpy as np
from skimage.color import rgb2lab, lab2rgb
from noise import pnoise2


# Color Adjustment: Change the color of original image to match some specific color style (defined by user in `model.txt`).
def adjust_color(image, model_path="./model/model.txt", style=-1):
    image = image.astype(np.float32) / 255.0
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_lab = rgb2lab(image_rgb)
    mean, std = cv2.meanStdDev(image_lab)
    mean = mean.flatten()
    std = std.flatten()

    centers = np.loadtxt(model_path, dtype=np.float32)

    # style -1 means to find the closest style
    if style == -1:
        source_vec = np.concatenate([mean, std])
        dists = np.linalg.norm(centers - source_vec, axis=1)
        best_style = np.argmin(dists)
    else:
        best_style = style

    target_mean = centers[best_style, :3]
    target_std = centers[best_style, 3:]

    # Reinhard color transfer
    image_adj = (image_lab - mean) * (target_std / std) + target_mean
    image_rgb = lab2rgb(image_adj).astype(np.float32)
    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
    image_bgr = np.clip(image_bgr * 255, 0, 255).astype(np.uint8)
    return image_bgr


# Saliency Distance Field: Perform saliency detection to identify the regions that are likely to be emphasized.
def get_binary_saliency_map(image):
    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    _, saliencyMap = saliency.computeSaliency(image)
    saliencyMap = cv2.normalize(saliencyMap, None, 0, 255, cv2.NORM_MINMAX).astype(
        np.uint8
    )
    saliencyMap = cv2.GaussianBlur(saliencyMap, (11, 11), 0)
    _, binary = cv2.threshold(
        saliencyMap, 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
    )
    return binary


def compute_saliency_distance_field(image):
    # compute saliency map
    binary_saliency = get_binary_saliency_map(image)

    # invert to get the background
    binary_inv = cv2.bitwise_not(binary_saliency)
    dist = cv2.distanceTransform(binary_inv, cv2.DIST_L2, 5)
    norm_dist = cv2.normalize(dist, None, 0, 1.0, cv2.NORM_MINMAX)

    # recursive Gaussian filter to smooth
    for _ in range(10):
        norm_dist = cv2.GaussianBlur(norm_dist, (5, 5), 0)

    return binary_saliency, norm_dist


# Abstraction: Regions that need not be emphasized will be simplified by abstraction, make variations in salient regions will be better preserved while the surrounding regions will tend to be abstracted.


def abstraction(image_bgr, dist_field, saliency_mask):
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB).astype(np.uint8)
    image_lab = rgb2lab(image_rgb)
    hfs = cv2.hfs.HfsSegment.create(
        image_rgb.shape[0], image_rgb.shape[1], 0.02, 30, 0.08, 60, 0.3, 32, 16
    )
    segments_rgb = hfs.performSegmentCpu(image_rgb, True)
    segments = cv2.cvtColor(segments_rgb, cv2.COLOR_RGB2GRAY).astype(np.uint8)

    H, W = image_rgb.shape[:2]
    output_lab = image_lab.copy()

    for i in range(H):
        for j in range(W):
            seg_id = segments[i, j]
            d = dist_field[i, j]

            if saliency_mask[i, j]:
                # for salient pixels, use a fixed kernel size of 5
                k = 5
            else:
                # for non-salient pixels, use a larger kernel size clamp(5*2*(d + 0.3), 4, 9)
                k = int(np.clip(5 * 2 * (d + 0.3), 4, 9))
                if k % 2 == 0:
                    k += 1

            half_k = k // 2

            # define local window bounds
            i1 = max(i - half_k, 0)
            i2 = min(i + half_k + 1, H)
            j1 = max(j - half_k, 0)
            j2 = min(j + half_k + 1, W)

            patch_lab = image_lab[i1:i2, j1:j2]
            patch_seg = segments[i1:i2, j1:j2]

            mask = patch_seg == seg_id

            if not saliency_mask[i, j]:
                center_color = image_lab[i, j][np.newaxis, np.newaxis, :]
                diff = np.linalg.norm(patch_lab - center_color, axis=-1)  # (k, k)
                neighbor_mask = (patch_seg != seg_id) & (diff < 0.3 * d)
                mask = mask | neighbor_mask

            if np.any(mask):
                # mean over all qualifying pixels in patch
                output_lab[i, j] = patch_lab[mask].mean(axis=0)

    result_rgb = lab2rgb(output_lab)
    result_bgr = cv2.cvtColor((result_rgb * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
    return segments, result_bgr


# Compute Boundaries: Region boundaries are classified into the following three groups with regard to the application of the hand tremor effect.
# 1. Boundaries in a wet-in-wet area should not be distorted;
# 2. Boundaries of regions with similar hues are distorted without overlaps and gaps;
# 3. The other boundaries are distorted with overlaps and gaps, as supposed to be generated by the hand tremor effect.
def angle(a, b):
    return min(abs(int(a) - int(b)), 180 - abs(int(a) - int(b)))


def boundary_classification(image_bgr, saliency_map):
    h, w = image_bgr.shape[:2]
    image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)

    # compute gradients
    grad_x = cv2.Scharr(image_gray, cv2.CV_16S, 1, 0)
    grad_y = cv2.Scharr(image_gray, cv2.CV_16S, 0, 1)

    abs_grad_x = cv2.convertScaleAbs(grad_x)
    abs_grad_y = cv2.convertScaleAbs(grad_y)

    gradient = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)
    _, boundary = cv2.threshold(gradient, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    result = np.zeros_like(boundary, dtype=np.uint8)

    for i in range(h):
        for j in range(w):
            if boundary[i, j] == 255:
                dx = int(grad_x[i, j])
                dy = int(grad_y[i, j])
                mag = np.hypot(dx, dy)
                if mag < 1e-5:
                    continue
                dx = int(3 * dx / mag)
                dy = int(3 * dy / mag)

                front = (j + dx, i + dy)
                back = (j - dx, i - dy)

                if not (
                    0 <= front[0] < w
                    and 0 <= front[1] < h
                    and 0 <= back[0] < w
                    and 0 <= back[1] < h
                ):
                    continue

                front_h, front_s, front_v = image_hsv[front[1], front[0]]
                back_h, back_s, back_v = image_hsv[back[1], back[0]]
                d = saliency_map[i, j]

                # Boundaries in a wet-in-wet area should not be distorted;
                if (d < 0.3 and angle(front_h, back_h) < 10) or (
                    d >= 0.3 and angle(front_h, back_h) < 45
                ):
                    result[i, j] = 1
                # Boundaries of regions with similar hues are distorted without overlaps and gaps;
                elif (
                    angle(front_h, back_h) < 45
                    or min(front_s, back_s) < 45
                    or min(front_v, back_v) < 45
                ):
                    for a in range(-3, 4):
                        for b in range(-3, 4):
                            y, x = i + a, j + b
                            if 0 <= y < h and 0 <= x < w:
                                if result[y, x] != 1:
                                    result[y, x] = 2
                # The other boundaries are distorted with overlaps and gaps, as supposed to be generated by the hand tremor effect.
                else:
                    for a in range(-3, 4):
                        for b in range(-3, 4):
                            y, x = i + a, j + b
                            if 0 <= y < h and 0 <= x < w:
                                if result[y, x] not in [1, 2]:
                                    result[y, x] = 3
    return result, grad_x, grad_y


# Wet-in-Wet Effect: Artists paint with a wet brush very a freshly painted wet region so that the pigments are mixed to produce feather-like patterns along a region boundary.


def wet_in_wet(
    image_bgr, boundary_map, grad_x, grad_y, saliency_map, max_distance=3.0, n_step=4
):
    h, w = image_bgr.shape[:2]
    image_float = image_bgr.astype(np.float32)
    image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)
    output = image_float.copy()
    rng = np.random.default_rng()

    # scattering noise pixels around the boundary on the brighter side.
    for y in range(h):
        for x in range(w):
            if boundary_map[y, x] == 1:
                dx = int(grad_x[y, x])
                dy = int(grad_y[y, x])

                if abs(grad_x[y, x]) >= 10 and abs(grad_y[y, x]) >= 10:
                    continue

                norm = np.hypot(dx, dy)
                if norm < 1e-5:
                    continue

                fx, fy = int(x + dx * 3 / norm), int(y + dy * 3 / norm)
                bx, by = int(x - dx * 3 / norm), int(y - dy * 3 / norm)
                if not (0 <= fx < w and 0 <= fy < h and 0 <= bx < w and 0 <= by < h):
                    continue

                hue_front = image_hsv[fy, fx, 0]
                hue_back = image_hsv[by, bx, 0]
                dh = angle(hue_front, hue_back)
                is_salient = saliency_map[y, x] > 0

                if (is_salient and dh >= 20) or (not is_salient and dh >= 90):
                    continue

                back_color = image_float[by, bx]
                d = rng.uniform(0, max_distance)
                sx, sy = int(x + dx * d / norm), int(y + dy * d / norm)
                if 0 <= sx < w and 0 <= sy < h:
                    output[sy, sx] = back_color

    # filtering along the normal vectors of the boundary to generate a feather-like pattern.
    base_kernel = np.zeros((15, 15), dtype=np.float32)
    for i in range(8):
        base_kernel[7, i] = (i + 1) / 8.0
        base_kernel[6, i] = (i + 1) / 16.0
        base_kernel[8, i] = (i + 1) / 16.0
    for i in range(8, 15):
        base_kernel[7, i] = (15 - i) / 8.0
        base_kernel[6, i] = (15 - i) / 16.0
        base_kernel[8, i] = (15 - i) / 16.0

    finished = np.zeros((h, w), dtype=np.uint8)
    kernel_cache = {}

    for y in range(h):
        for x in range(w):
            if boundary_map[y, x] != 1:
                continue

            dx = int(grad_x[y, x])
            dy = int(grad_y[y, x])

            if abs(grad_x[y, x]) >= 10 and abs(grad_y[y, x]) >= 10:
                continue

            norm = np.hypot(dx, dy)
            if norm < 1e-5:
                continue

            angle_key = int(round(angle(dx, dy) / 10) * 10)

            if angle_key not in kernel_cache:
                M = cv2.getRotationMatrix2D((7, 7), angle_key, 1)
                kernel_cache[angle_key] = cv2.warpAffine(base_kernel, M, (15, 15))

            rotated_kernel = kernel_cache[angle_key]

            for step in range(n_step):
                px = int(x + dx * step / norm)
                py = int(y + dy * step / norm)
                if not (0 <= px < w and 0 <= py < h):
                    continue
                if finished[py, px]:
                    continue
                finished[py, px] = 1

                if 7 <= px < w - 7 and 7 <= py < h - 7:
                    norm_kernel = rotated_kernel / (
                        np.sum(rotated_kernel) + np.finfo(np.float32).eps
                    )
                    for c in range(3):
                        roi = output[py - 7 : py + 8, px - 7 : px + 8, c]
                        blurred = cv2.filter2D(roi, -1, norm_kernel)
                        output[py, px, c] = blurred[7, 7]

    return np.clip(output, 0, 255).astype(np.uint8)


# Hand Tremor Effect: A watercolorist hardly draws straight lines without breaks or wiggling because human muscles can hardly be controlled accurately due to the noise in the human nervous system.
def get_perlin_noise(height, width, scale=0.1, seed=0):
    noise_image = np.zeros((height, width), dtype=np.float32)

    for y in range(height):
        for x in range(width):
            noise_val = pnoise2(
                x * scale, y * scale, repeatx=width, repeaty=height, base=seed
            )
            noise_image[y, x] = noise_val

    return noise_image


def hand_tremor(image_bgr, segments, boundary_map, perlin_generator):
    h, w = image_bgr.shape[:2]
    image_bgr = image_bgr.copy()
    output = image_bgr.copy()

    # generate 4 sets of perlin noise for displacement vectors
    def generate_noise_set():
        noise1 = np.zeros((h, w), dtype=np.float32)
        noise2 = np.zeros((h, w), dtype=np.float32)
        noise3 = np.zeros((h, w), dtype=np.float32)
        noise4 = np.zeros((h, w), dtype=np.float32)
        base = 0.7
        div_sum = 0.0

        for _ in range(8):
            full_noise = perlin_generator(h * 2, w * 2, base)
            base /= 2
            noise1 += full_noise[0:h, 0:w] / base
            noise2 += full_noise[h : 2 * h, 0:w] / base
            noise3 += full_noise[0:h, w : 2 * w] / base
            noise4 += full_noise[h : 2 * h, w : 2 * w] / base
            div_sum += 1.0 / base

        for noise in [noise1, noise2, noise3, noise4]:
            noise /= div_sum
            cv2.normalize(
                noise, noise, alpha=-1.35 + 7, beta=2.35 - 7, norm_type=cv2.NORM_MINMAX
            )

        return noise1, noise2, noise3, noise4

    n1, n2, n3, n4 = generate_noise_set()

    for y in range(h):
        for x in range(w):
            b_type = boundary_map[y, x]

            # distorted without overlaps and gaps
            if b_type == 2:
                dx, dy = int(n1[y, x]), int(n2[y, x])
                tx, ty = x + dx, y + dy
                if 0 <= tx < w and 0 <= ty < h:
                    output[y, x] = image_bgr[ty, tx]

            # distorted with overlaps and gaps
            elif b_type == 3:
                dx1, dy1 = int(n1[y, x]), int(n2[y, x])
                dx2, dy2 = int(n3[y, x]), int(n4[y, x])
                tx1, ty1 = x + dx1, y + dy1
                tx2, ty2 = x + dx2, y + dy2

                color1 = image_bgr[y, x].astype(np.float32)
                color2 = image_bgr[y, x].astype(np.float32)

                if 0 <= tx1 < w and 0 <= ty1 < h:
                    if segments[y, x] == segments[ty1, tx1]:
                        color1 = image_bgr[ty1, tx1]

                if 0 <= tx2 < w and 0 <= ty2 < h:
                    if segments[y, x] == segments[ty2, tx2]:
                        color2 = image_bgr[ty2, tx2]

                mix = 0.5 * color1 + 0.5 * color2
                output[y, x] = mix.astype(np.uint8)

    return np.clip(output, 0, 255).astype(np.uint8)


# Other Effects


# Edge Darkening: Darkened stroke boundaries due to water evaporation
def edge_darkening(image_bgr):
    image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    grad_x = cv2.Scharr(image_gray, cv2.CV_16S, 1, 0)
    grad_y = cv2.Scharr(image_gray, cv2.CV_16S, 0, 1)
    abs_grad_x = cv2.convertScaleAbs(grad_x)
    abs_grad_y = cv2.convertScaleAbs(grad_y)

    edge = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)
    _, edge = cv2.threshold(edge, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    blurred_edge = cv2.blur(edge, (5, 5))
    dilated_edge = cv2.dilate(blurred_edge, np.ones((5, 5), np.uint8), iterations=2)
    blurred_edge = cv2.blur(dilated_edge, (11, 11))
    edge = cv2.addWeighted(edge, 0.1, blurred_edge, 0.7, 0)
    edge = cv2.GaussianBlur(edge, (5, 5), 0)

    edge = edge.astype(np.float32) / 255.0
    edge *= 0.3
    edge = 1.1 - edge

    edges = np.stack([edge] * 3, axis=-1)
    image_float = image_bgr.astype(np.float32) / 255.0
    darkened = image_float * edges
    return np.clip(darkened * 255, 0, 255).astype(np.uint8)


# Granulation: High-frequency boundary distortion due to rough paper surface;
def granulation(image_bgr):
    h, w = image_bgr.shape[:2]
    image = image_bgr.astype(np.float32) / 255.0

    noise_texture = np.zeros((h, w), dtype=np.float32)
    noise_texture += get_perlin_noise(h, w, 0.5) * 4
    noise_texture += get_perlin_noise(h, w, 0.25) * 2
    noise_texture += get_perlin_noise(h, w, 0.125) * 1
    noise_texture /= 7.0

    noise_texture = noise_texture * 0.15 + 0.95

    noise_textures = np.stack([noise_texture] * 3, axis=-1)

    image *= noise_textures
    return np.clip(image * 255, 0, 255).astype(np.uint8)


# Turbulence flow: Low-frequency pigment separation due to uneven water density.
def turbulence_flow(image_bgr, get_perlin_noise):
    h, w = image_bgr.shape[:2]
    image = image_bgr.astype(np.float32) / 255.0
    noise_texture = np.zeros((h, w), dtype=np.float32)
    base = 0.1
    div_sum = 0.0

    for _ in range(8):
        noise = get_perlin_noise(h, w, base)
        noise_texture += noise / base
        div_sum += 1.0 / base
        base /= 2

    noise_texture /= div_sum
    noise_texture += 0.995

    noise_textures = np.stack([noise_texture] * 3, axis=-1)

    image *= noise_textures
    return np.clip(image * 255, 0, 255).astype(np.uint8)


# Anti-Aliasing
def antialiasing(image_bgr):
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    edges = cv2.Laplacian(gray, cv2.CV_8U, ksize=3)
    _, mask = cv2.threshold(edges, 20, 255, cv2.THRESH_BINARY)
    blurred = cv2.GaussianBlur(image_bgr, (3, 3), sigmaX=0.5)
    result = np.where(mask[..., None] > 0, blurred, image_bgr)
    return result
